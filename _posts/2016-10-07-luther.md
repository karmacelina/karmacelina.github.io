---
layout: post
title: Project Luther: How might an actor/actress increase their exposure?
---

To start: 
[Important lesson](https://lh3.googleusercontent.com/proxy/ijPj85FBf8df4zzfD-i0RjNLCoAEVdzZSCp0LXSrH8HixlpZIkHY51_zNsYfNK5s5pySNbno-QddcYvvLM5dfTLbouO0N3C8ILWG2b928hvyOBeJkDaGW9F7xfaUrtXI=w426-h329-p)

Luther was a two-week project that required web scraping, exploratory data analysis and using linear regression / decision trees / random forest machine learning models. 

Here's the (disorganized) [repo](https://github.com/karmacelina/Luther_v0) containing my work. I expect to clean up my code and perform more data cleaning and modeling as time becomes available ...

# Project Luther: Workflow

## From scraping to placing the data in pandas dataframes: 

* First, I scraped all movie data from [Box Office Mojo](http://www.boxofficemojo.com/) using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/). 
	* It would be useful to learn HTML and CSS. 
* When scraping the data, it was VERY useful to define functions that used regular expressions (a.k.a regex) to clean the data that would ultimatedly be placed in a dictionary. Both [Regexr](http://regexr.com/) and [Pythex](http://pythex.org/) were very handy-dandy tools to use to check my regex expressions.  
* After scraping all the data and placing it in a dictionary, I placed the data in a pandas DataFrame. (We'll learn SQL next week!)
	* Scraping was a tedious process; it was important to write code that would not abort the whole scraping process in case the BOM site caught on to my scraping efforts. I ended up with six different dictionaries, so, six different data frames, that I concatenated together before moving on with more data cleaning and exploratory data analysis. 

## Exploratory data analysis using pandas, matplotlib, and seaborn

* continue writing ... 
 